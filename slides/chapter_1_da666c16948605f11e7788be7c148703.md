---
title: Insert title here
key: da666c16948605f11e7788be7c148703

---
## Lesson 3.1: Workings and some theory of AR model

```yaml
type: "TitleSlide"
key: "a8666f30be"
```

`@lower_third`

name: Gourav S B
title: Instructor at DataCamp


`@script`
Hi there! I am Gourav and today we will learn about the workings and some theory of AR or Auto-regression model.


---
## Lesson Objectives:

```yaml
type: "FullSlide"
key: "e2e5569f67"
center_content: true
```

`@part1`
1. AR Model and its mathematical form
2. Assumptions behind AR modelling
3. Apply AR model in python
4. Use ACF in AR model


`@script`
After this lesson you will be 
1) able to describe an auto-regression model with its mathematical equation,
2) understand the various assumptions required for validating an implementation of AR model,
3) Learn how to apply the AR model in python 
4) finally use ACF() learnt in the previous lesson to determine the significant timelags.


---
## Theory of AR Model

```yaml
type: "FullSlide"
key: "79bc718a82"
center_content: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/6ec14fc97a7538d4efb7f5a237df61247994fe13/Screenshot%202019-01-03%20at%2012.43.18%20AM.png) 
NOTE: It is not necessary for the time series to be stationary to apply AR model.


`@script`
Let's begin by understanding the theory of AR model. 

A time series data that has a set of data points that linearly depend upon its past instances can use the Auto-Regression Model to predict the future instances.

Such a process can be written as shown here.

Where **Xt** is the value to be predicted at time "t",
      **Xt-1** is the value at time "t-1", so on and
      **phis'** are the weights that the model will predict, which are to be multiplied with the corresponding previous time lag values.

Et is the error term or noise at time "t".

And we are going to consider "p" previous time lags which needs to be determined and supplied to the model.

In short, we have "**a sum of weighted previous time lags**" to forecast the future values. 

Interestingly, the time series do not need to be stationary to apply AR model, so we need not worry about the stationary test learnt earlier in the past lesson.


---
## Calculation of significant lag values "P" in a AR(P) model

```yaml
type: "FullSlide"
key: "f01bab6a05"
```

`@part1`
```python
from statsmodels.tsa.ar_model import AR
from sklearn.metrics import mean_squared_error
df = pd.read_csv("jj.data") # Import the JJ data
X = df.values
nobs = len(X) # Number of observations
train, test = X[1:nobs-8], X[nobs-8:]
model = AR(train) # train an AutoRegression model

default_lag = round(12*(nobs/100.)**(1/4.))

model_fit = model.fit()
print ("Lag in model_fit: %s" % model_fit.k_ar)
model_fit_1 = model.fit(maxlag=8)
print ("Lag in model_fit_1: %s" % model_fit_1.k_ar)
model_fit_2 = model.fit(maxlag=8,ic="aic")
print ("Lag in model_fit_2: %s" % model_fit_2.k_ar)

Lag in model_fit: 11
Lag in model_fit_1: 8
Lag in model_fit_2: 5

https://www.statsmodels.org/dev/generated/statsmodels.tsa.ar_model.AR.fit.html#statsmodels.tsa.ar_model.AR.fit.html
```


`@script`
Let's begin by finding the value of "P-the number of previous lags" in the AR(P) model using the statsmodel package.

We will be using the Johnson&Johnson-data to understand this concept. 

First, we extract the return values of JJ-data and split it into train and test set. The last 8 quarters will be our test set. 

Next we will train the AR object on the train data using the function AR() imported from the statsmodel.tsa package. 

Next we will learn how to calculate the significant number of time lags, ie which past instances have a significant ripple effect on the future instances. For this, we have two main parameters (maxlag - "maximum lag" and ic - "information criteria") which govern what will be the allowed number of time lags for the model to be built.

We will see three methods to find the value of "p":
1) In case when the 2 parameters are missing, the default value of "p" is calculated as shown here. Interestingly, "p" is directly proportional to the nobs, so more observations means more significant time lags will be added to the model. In our case, 11 is the default value.

2)Next we can also specify the value of "p" using the maxlag parameter as shown here.

3)Finally we can let the model decide the value of "p". As here maxlag value is altered between 1 to 8, while measuring the error using the aic or Akaike Information Criterion. Here we get 5 as significant number of time lags. There are other values of the parameter "ic" which you can explore here.


---
## Prediction of quarterly returns for last 2 years using AR(11) model

```yaml
type: "FullCodeSlide"
key: "82b866b3e9"
```

`@part1`
```python
predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, 
dynamic=False) # make predictions

for i in range(4): # Looking at the first year prediction
    print("predicted=%f, expected=%f" % (predictions[i], test[i]))

print("Test MSE: %.3f" % mean_squared_error(test, predictions))

print("Coefficients: %s" % model_fit.params)

pyplot.plot(test,label="Actual")
pyplot.plot(predictions, color="red",label="Predicted")
pyplot.ylabel("Quarterly Returns($)")
pyplot.xlabel("Quarter")
pyplot.show()
```
```python
Coefficients: [ 0.05739753  0.13081868  0.22824468  0.11240213  0.87438022 
-0.32461781 0.104462 -0.0935553....]
predicted=14.095006, expected=14.040000
predicted=13.896251, expected=12.960000
predicted=13.574049, expected=14.850000
predicted=10.463847, expected=9.990000
Test MSE: 0.600
```


`@script`
Now we will predict the return values of the last 2 years using the AR(11) model with 11 significant previous time lags using the model_fit object we trained earlier.

As these values are sequentially ordered we pass the values to the predict function with the help of this code.

Using the mean_squared_error as the measure we get 0.6, which means our prediction has a very low error.

One interesting thing to note here is that all the 11 values of phi estimated, will always be between -1 and 1, which means only a part of the past signal is carried forward.

Here the next block of code gives us the plot for the actual and the predicted values for the last eight quarters.


---
## Plot comparing the actual  and predicted quarterly returns for the year 1979 and 1980

```yaml
type: "FullSlide"
key: "432fa90067"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/8332371ffb092b70b5a14d2c607476f2febebdb2/lesson_3_1.png)


`@script`
Here is the plot comparing the actual and predicted returns.The blue line is actual return and the red line is the predicted return. It seems the model has done a good job in learning the trend from the past returns!!


---
## How to use ACF to better estimate the significant time lags

```yaml
type: "FullCodeSlide"
key: "2976f30bd6"
```

`@part1`
```python
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.stattools import acf
plot_acf(df, lags=12)
plot_acf(model_fit.predict(start=12,end=79,dynamic=False), lags=12)
pyplot.show()

ACF_df = pd.DataFrame(columns=["ACF from actual data","ACF from fitted model"])
ACF_df["ACF from actual data"] = acf(df,nlags=12)
ACF_df["ACF from fitted model"] = acf(model_fit.predict(start=12,end=80,
dynamic=False),nlags=12)
print (ACF_df)
    ACF from actual data  ACF from fitted model
0               1.000000               1.000000
1               0.925102               0.894762
2               0.888263               0.817167
3               0.832848               0.808366
4               0.824077               0.788722
5               0.763800               0.698282
6               0.717595               0.634483
7               0.675024               0.620639
8               0.654356               0.596962
9               0.608335               0.515699
10              0.564152               0.472320
11              0.525560               0.463515
12              0.500414               0.438432
```


`@script`
Remember we learnt Auto correlated function, to calculate the correlation b/w the time lags at specific points t,t+1,t+2 and so on wrt "t". We can use the ACF to help us estimate the significant time lags to be considered for AR model as well. 

Let's try this approach and estimate the time lags for JJ data prediction. We can use the plot_acf() from statsmodels package to plot the acf.

We will print the acf both from the actual data and the predicted data from the AR model.

1) First we pass the actual data to plot_acf() specifying the lags to be considered, (here I have supplied 12). 
2) Second we pass the predicted values using model_fit.
Then I have summarised the output in a dataframe called ACF_df as shown here.

We can see the correlation falls below 0.50 around 11 timelags for the actual dataset. Interestingly, for the fitted model it falls quickly as model we built is an approximation of the actual data.


---
## ACF plot for the data and fitted model

```yaml
type: "FullImageSlide"
key: "e2cad4c24c"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/b2523944cd8748a7481a4d18dd72872a61932668/Screenshot%202019-01-03%20at%201.37.58%20AM.png)


`@script`
Now here is ACF plot for the two data sets. Evidently suggesting 11 time lags is the correct choice with 0.50 threshold.


---
## Let's fit some AR model!

```yaml
type: "FinalSlide"
key: "7d7d9a328f"
assignment: "ACF & AR model and how they can be used together to understand time series data."
```

`@script`
Now let's fit some AR model by doing the following excerises!

