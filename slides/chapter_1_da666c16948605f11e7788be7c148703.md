---
title: Insert title here
key: da666c16948605f11e7788be7c148703

---
## Lesson 3.1: Workings and some theory of AR model

```yaml
type: "TitleSlide"
key: "a8666f30be"
```

`@lower_third`

name: Gourav S B
title: Instructor at DataCamp


`@script`
Hi there! I am Gourav and today we will learn about the workings and some theory of AR or Auto-regression model.


---
## Lesson Objectives:

```yaml
type: "FullSlide"
key: "e2e5569f67"
center_content: true
```

`@part1`
1. AR Model and its mathematical form
2. Assumptions behind AR modelling
3. Apply AR model in python
4. Use ACF in AR model


`@script`
After this lesson you will be 
1) able to describe an auto-regression model with its mathematical equation,
2) understand the various assumptions required for validating an implementation of AR model,
3) Learn how to apply the AR model in python 
4) finally use ACF() learnt in the previous lesson to determine the significant timelags.


---
## Theory of AR Model

```yaml
type: "FullSlide"
key: "79bc718a82"
center_content: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/6ec14fc97a7538d4efb7f5a237df61247994fe13/Screenshot%202019-01-03%20at%2012.43.18%20AM.png) 
NOTE: It is not necessary for the time series to be stationary to apply AR model.


`@script`
Let's begin by understanding the theory of AR model. 

If we have a time series data which depends linearly on its past instances then we can model such a data using Auto-regression model. Essentially the value of the future instances are governed by past values.

The mathematical notation of such process can be written as shown here. Where Xt is the value at instant time "t" and phis' are the weights that the model will predict, which are to be multiplied with the corresponding previous time lag values Xt-1 and so on.Et is the error term associated with the time series.Essentially, we have "**a sum of weighted previous time lags**" to forecast the future values. 

Interestingly, the time series do not need to be stationary to apply AR model so we need not worry about the stationary test learnt earlier in the past lesson.


---
## Using AR model to predict JJ shares quarterly returns

```yaml
type: "FullCodeSlide"
key: "413b57aa92"
```

`@part1`
First, lets view the JJ shares data and print the co-relation for 1 time lag using pandas corr() function
```python
import pandas as pd
# Reading the Johnson-&-Johnson shares quarterly returns data
df = pd.read_csv("jj.data")
df["date"] = pd.to_datetime(df["date"])# Convert date to pandas datetime object
df.set_index("date",inplace=True) # Set index of df as date
print(df.head(4)) # View the structured data of 1960
            return
date             
1960-03-31   0.71
1960-06-30   0.63
1960-09-30   0.85
1960-12-31   0.44
```

```python
shifted_df = pd.concat([df["return"].shift(1), df["return"]], axis=1)
shifted_df.columns = ["return at t-1", "return at t+1"]
print(shifted_df.corr())
              return at t-1  return at t+1
return at t-1      1.000000      0.944936
return at t+1      0.944936      1.000000
```


`@script`
Lets begin by applying the AR model to the JJ data.

First, let's load and view the JJ data in the pandas data frame using this piece of code. The JJ data has two columns; one for the quarter dates for 20 years and second for returns realised in dollars. We load the data using pandas and convert the date column to timeseries object followed by setting it as index. So the head of data looks as shown here.

Now we want to see the correlation in values with one time lag ie. upon shifting the "return" column by one row up what is the correlation between the values at (t-1) and (t+1). We can use the inbuilt function corr() to find the correlation. Surprisingly, it turns out to be 0.94 which means they are highly co-related.


---
## Lag calculation within AR.fit() method of "statsmodels" package

```yaml
type: "FullSlide"
key: "f01bab6a05"
```

`@part1`
```python
from statsmodels.tsa.ar_model import AR
from sklearn.metrics import mean_squared_error

X = df.values
nobs = len(X) # Number of observations
train, test = X[1:nobs-8], X[nobs-8:]
model = AR(train) # train an AutoRegression model

# The internal formula inside the AR.fit() method to
# calculate default lag in case maxlag and ic parameter are missing
default_lag = round(12*(nobs/100.)**(1/4.))
print ("Default lag value %f as obtained from formula" % default_lag)
# Fit the unconditional maximum likelihood of an AR(p) process
model_fit = model.fit()
print ("Lag in model_fit: %s" % model_fit.k_ar)
model_fit_1 = model.fit(maxlag=8)
print ("Lag in model_fit_1: %s" % model_fit_1.k_ar)
model_fit_2 = model.fit(maxlag=8,ic="aic")
print ("Lag in model_fit_2: %s" % model_fit_2.k_ar)

Default lag value 11.0 as obtained from formula
Lag in model_fit: 11
Lag in model_fit_1: 8
Lag in model_fit_2: 5
```


`@script`
Now lets apply AR model available in the statsmodel package of python. For that we import the required model statsmodel.tsa.ar_model and we will be using the mean_squared_error to measure how well our predictions are.

We extract the return values of dataframe and split it to train and test set here. The last two years or 8 quarters is our testset. Next we initiate the AR object on the train data. 

In the next block of code we will try to understand how to find the significant time lags to be used for our train set.There are parameters (maxlag and ic) which govern what will be the allowed number of time lags for the model.

First, lets look at how the default internal lag value is calculated in the AR in case when the two parameters are missing. The formula is shown here. Essentially time lag is directly proportional to the nobs, so more observations means more significant time lags will be added to the model. Here we get 11 as the default value.

2) We can also specify the maxlag parameter as shown here.

Finally we can let the model fit the data for uses all the values between 1 to maxlag=8 are tried, and based on error measure such as "Akaike Information Criterion" we get 5 as significant number of time lags.


---
## Predict quarterly returns for last two years using AR(11) model

```yaml
type: "FullCodeSlide"
key: "82b866b3e9"
```

`@part1`
```python
print("Coefficients: %s" % model_fit.params)
predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, 
dynamic=False) # make predictions
for i in range(len(predictions)):
    print("predicted=%f, expected=%f" % (predictions[i], test[i]))
error = mean_squared_error(test, predictions)
print("Test MSE: %.3f" % error)
pyplot.plot(test,label="Actual")
pyplot.plot(predictions, color="red",label="Predicted")
pyplot.ylabel("Quarterly Returns($)")
pyplot.xlabel("Quarter")
pyplot.show() # plot the prediction
```

```python
Lag in model_fit: 11
Coefficients: [ 0.05739753  0.13081868  0.22824468  0.11240213  0.87438022 
-0.32461781 0.104462 -0.0935553   0.45528992 -0.05165633 -0.40466646  0.057615]
predicted=14.095006, expected=14.040000
predicted=13.896251, expected=12.960000
predicted=13.574049, expected=14.850000
predicted=10.463847, expected=9.990000
predicted=17.347931, expected=16.200000
predicted=15.504115, expected=14.670000
predicted=15.822432, expected=16.020000
predicted=11.725124, expected=11.610000
Test MSE: 0.600
```


`@script`
Using the model_fit that we trained earlier we can predict the values as shown here. Since these values are sequentially ordered we pass the start as the length of train and end as length of train plus test. Using the mean_squared_error measure we get 0.6 which is fairly a very good prediction.
We can also print the weights or phi in our model equation using the model_fit.params command. 

Note that phi is always between -1 and 1 which means only part of the past signal is retained in the future instances.


---
## Plot comparing the actual  and predicted quarterly returns for the year 1979 and 1980

```yaml
type: "FullSlide"
key: "432fa90067"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/8332371ffb092b70b5a14d2c607476f2febebdb2/lesson_3_1.png)


`@script`
Here is the plot comparing the actual and predicted returns.The blue line is actual return and the red line is the predicted return. It seems the model has done a good job in learning the trend from the past returns!!


---
## How to use ACF to better estimate the significant time lags

```yaml
type: "FullCodeSlide"
key: "2976f30bd6"
```

`@part1`
```python
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.stattools import acf
plot_acf(df, lags=12)
plot_acf(model_fit.predict(start=12,end=79,dynamic=False), lags=12)
pyplot.show()

ACF_df = pd.DataFrame(columns=["ACF from actual data","ACF from fitted model"])
ACF_df["ACF from actual data"] = acf(df,nlags=12)
ACF_df["ACF from fitted model"] = acf(model_fit.predict(start=12,end=80,
dynamic=False),nlags=12)
print (ACF_df)
    ACF from actual data  ACF from fitted model
0               1.000000               1.000000
1               0.925102               0.894762
2               0.888263               0.817167
3               0.832848               0.808366
4               0.824077               0.788722
5               0.763800               0.698282
6               0.717595               0.634483
7               0.675024               0.620639
8               0.654356               0.596962
9               0.608335               0.515699
10              0.564152               0.472320
11              0.525560               0.463515
12              0.500414               0.438432
```


`@script`
Remember we learnt Auto correlated function, to calculate the correlation b/w the time lags at specific points t,t+1,t+2 and so on wrt "t". We can use the ACF to help us estimate the significant time lags to be considered for AR model as well. 

Let's try this approach and estimate the time lags for JJ data prediction. We can use the plot_acf() from statsmodels package to plot the acf.

We will print the acf both from the actual data and the predicted data from the AR model.

1) First we pass the actual data to plot_acf() specifying the lags to be considered, (here I have supplied 12). 
2) Second we pass the predicted values using model_fit.
Then I have summarised the output in a dataframe called ACF_df as shown here.

We can see the correlation falls below 0.50 around 11 timelags for the actual dataset. Interestingly, for the fitted model it falls quickly as model we built is an approximation of the actual data.


---
## ACF plot for the data and fitted model

```yaml
type: "FullImageSlide"
key: "e2cad4c24c"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4370/datasets/b2523944cd8748a7481a4d18dd72872a61932668/Screenshot%202019-01-03%20at%201.37.58%20AM.png)


`@script`
Now here is ACF plot for the two data sets. Evidently suggesting 11 time lags is the correct choice with 0.50 threshold.


---
## Let's fit some AR model!

```yaml
type: "FinalSlide"
key: "7d7d9a328f"
assignment: "ACF & AR model and how they can be used together to understand time series data."
```

`@script`
Now let's fit some AR model by doing the following excerises!

